{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T01:50:52.275784Z",
     "iopub.status.busy": "2020-12-13T01:50:52.275000Z",
     "iopub.status.idle": "2020-12-13T01:50:52.286453Z",
     "shell.execute_reply": "2020-12-13T01:50:52.284667Z",
     "shell.execute_reply.started": "2020-12-13T01:50:52.275464Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import TokenSearcher\n",
    "from nltk.corpus import udhr\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet as wn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your Turn: Pick a language of interest in udhr.fileids(), and define a variable \n",
    "raw_text = udhr.raw(Language-Latin1). Now plot a frequency distribution of the letters \n",
    "of the text using nltk.FreqDist(raw_text).plot().\n",
    "\"\"\"\n",
    "raw_text = udhr.raw('Balinese-Latin1')\n",
    "plt.figure()\n",
    "nltk.FreqDist(raw_text).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Turn: Working with the news and romance genres from the Brown Corpus, find out which days of the week are most newsworthy, and which are most romantic. Define a variable called days containing a list of days of the week, i.e. ['Monday', ...]. Now tabulate the counts for these words using cfd.tabulate(samples=days). Now try the same thing using plot in place of tabulate. You may control the output order of days with the help of an extra parameter: samples=['Monday', ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "categories = ['news', 'romance']\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "                            (category,word)\n",
    "                            for category in categories\n",
    "                            for word in brown.words(categories = category))\n",
    "cfd.plot(samples=days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating random text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(cfdist, word, num=15):\n",
    "    for i in range(num):\n",
    "        print(word, end=' ')\n",
    "        word = cfdist[word].max()\n",
    "\n",
    "text = nltk.corpus.genesis.words('english-kjv.txt')\n",
    "bigrams = nltk.bigrams(text)\n",
    "cfd = nltk.ConditionalFreqDist(bigrams) \n",
    "generate_model(cfd, 'what')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-10T01:41:28.073589Z",
     "iopub.status.busy": "2020-12-10T01:41:28.072729Z",
     "iopub.status.idle": "2020-12-10T01:41:28.452321Z",
     "shell.execute_reply": "2020-12-10T01:41:28.449750Z",
     "shell.execute_reply.started": "2020-12-10T01:41:28.073189Z"
    }
   },
   "source": [
    "wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('act')\n",
    "wn.synset('act.v.02').hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(wn.all_synsets('n')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zipf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipf(text):\n",
    "    \"\"\"text is a list of tokens, or is otherwise transformed into one\"\"\"\n",
    "    if type(text) == str:\n",
    "        text = s.split()\n",
    "    \n",
    "    unique_words = set(text)\n",
    "    fdist = nltk.FreqDist(w.lower() for w in text)\n",
    "    ranks = [] #dict() #the inverse of fdist\n",
    "    for k in fdist.keys():\n",
    "        ranks.append(fdist[k])\n",
    "    ranks = sorted(ranks)\n",
    "    freqs = [1/r for r in ranks]\n",
    "    return ranks, freqs\n",
    "    \n",
    "s_arrs = []\n",
    "for alphabet in [\"ab \", \"abcdefg \", \"abcdefgheijkl \", \"abcdefghijklmnopqrstuvwxyz \"]:\n",
    "    s = \"\"\n",
    "    for i in range(10000):\n",
    "        s = s + random.choice(alphabet)\n",
    "    s_arrs.append(s)\n",
    "\n",
    "plt.figure()\n",
    "# for s in s_arrs:\n",
    "#     x,y = zipf(s)\n",
    "#     plt.loglog(x,y)\n",
    "# plt.show()\n",
    "x,y = zipf(s[1])\n",
    "plt.loglog(x,y)\n",
    "x,y = zipf(text)\n",
    "plt.loglog(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To extract findall from nltk.Text object, need to use [Token Searcher](https://stackoverflow.com/questions/34097316/findall-regular-expression-wont-assign-to-a-variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_tokens = brown.words(categories='news')\n",
    "#fdist = nltk.FreqDist(w.lower() for w in news_tokens)\n",
    "news_text = nltk.Text(news_tokens)\n",
    "x = TokenSearcher(news_text).findall(r'<as> <\\w*> <as> <\\w*>')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"When I'M a Duchess,' she said to herself, (not in a very hopeful tone\n",
    " though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n",
    " well without--Maybe it's always pepper that makes people hot-tempered\"\"\"\n",
    "re.findall(r'\\w+|\\S\\w*', raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: programming stuff\n",
    "\n",
    "also need to understand map, filter, dynamic programming, recursion more\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def traverse(graph, start, node):\n",
    "    graph.depth[node.name] = node.shortest_path_distance(start)\n",
    "    for child in node.hyponyms():\n",
    "        graph.add_edge(node.name, child.name) \n",
    "        traverse(graph, start, child)\n",
    "\n",
    "def hyponym_graph(start):\n",
    "    G = nx.Graph() \n",
    "    G.depth = {}\n",
    "    traverse(G, start, start)\n",
    "    return G\n",
    "\n",
    "def graph_draw(graph):\n",
    "    nx.draw(graph,\n",
    "         node_size = [16 * graph.degree(n) for n in graph],\n",
    "         node_color = [graph.depth[n] for n in graph],\n",
    "         with_labels = False)\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(figsize=[18,12])\n",
    "food = wn.synset('food.n.01')\n",
    "graph = hyponym_graph(food)\n",
    "graph_draw(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-13T02:45:26.697230Z",
     "iopub.status.busy": "2020-12-13T02:45:26.696455Z",
     "iopub.status.idle": "2020-12-13T02:45:26.711651Z",
     "shell.execute_reply": "2020-12-13T02:45:26.709872Z",
     "shell.execute_reply.started": "2020-12-13T02:45:26.696920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " (\"'m\", 'VBP'),\n",
       " ('going', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('bløngofolize', 'VB'),\n",
       " ('this', 'DT'),\n",
       " ('deempom', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('smuggobulop', 'NN')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"I'm going to bløngofolize this deempom with my smuggobulop\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-14T21:02:59.822761Z",
     "iopub.status.busy": "2020-12-14T21:02:59.821866Z",
     "iopub.status.idle": "2020-12-14T21:03:00.911637Z",
     "shell.execute_reply": "2020-12-14T21:03:00.910212Z",
     "shell.execute_reply.started": "2020-12-14T21:02:59.822364Z"
    }
   },
   "outputs": [],
   "source": [
    "brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')\n",
    "data = nltk.ConditionalFreqDist((word.lower(), tag)\n",
    "                                 for (word, tag) in brown_news_tagged)\n",
    "\n",
    "for word in sorted(data.conditions())[10000:10020]:\n",
    "    \n",
    "    if len(data[word]) > 3:\n",
    "        print(data[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-14T21:03:16.523602Z",
     "iopub.status.busy": "2020-12-14T21:03:16.522664Z",
     "iopub.status.idle": "2020-12-14T21:04:26.959375Z",
     "shell.execute_reply": "2020-12-14T21:04:26.957703Z",
     "shell.execute_reply.started": "2020-12-14T21:03:16.523388Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.app.concordance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
